#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
çŸ¥ä¹çƒ­æ¦œçˆ¬è™«ç³»ç»Ÿ - å¢å¼ºç‰ˆ
åŠŸèƒ½ï¼šå®šæ—¶ä»»åŠ¡ã€æ•°æ®åˆ†æã€å»é‡ã€æ‰©å±•å­—æ®µé‡‡é›†
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from apscheduler.schedulers.blocking import BlockingScheduler
from apscheduler.triggers.cron import CronTrigger
import time
import json
import os
import pickle
import hashlib
import re
from datetime import datetime, timedelta
from collections import defaultdict
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Optional, Tuple
import logging

# é…ç½®matplotlibä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class EnhancedZhihuCrawler:
    """å¢å¼ºç‰ˆçŸ¥ä¹çˆ¬è™« - æ”¯æŒå®šæ—¶ä»»åŠ¡ã€æ•°æ®åˆ†æã€å»é‡ç­‰åŠŸèƒ½"""
    
    def __init__(self, data_dir="data"):
        self.data_dir = data_dir
        self.cookie_file = os.path.join(data_dir, "zhihu_cookies.pkl")
        self.history_file = os.path.join(data_dir, "crawl_history.json")
        self.duplicate_file = os.path.join(data_dir, "question_hashes.json")
        
        # åˆ›å»ºå¿…è¦ç›®å½•
        for subdir in ["raw", "processed", "analysis", "reports"]:
            os.makedirs(os.path.join(data_dir, subdir), exist_ok=True)
        
        # é…ç½®æ—¥å¿—
        self._setup_logging()
        
        # åˆå§‹åŒ–å»é‡æ•°æ®
        self.question_hashes = self._load_question_hashes()
        
        self.driver = None
        
    def _setup_logging(self):
        """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_dir = os.path.join(self.data_dir, "logs")
        os.makedirs(log_dir, exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(os.path.join(log_dir, f"crawler_{datetime.now().strftime('%Y%m%d')}.log")),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def _load_question_hashes(self) -> set:
        """åŠ è½½å·²çŸ¥é—®é¢˜çš„å“ˆå¸Œå€¼ç”¨äºå»é‡"""
        if os.path.exists(self.duplicate_file):
            try:
                with open(self.duplicate_file, 'r', encoding='utf-8') as f:
                    return set(json.load(f))
            except:
                return set()
        return set()
    
    def _save_question_hashes(self):
        """ä¿å­˜é—®é¢˜å“ˆå¸Œå€¼"""
        with open(self.duplicate_file, 'w', encoding='utf-8') as f:
            json.dump(list(self.question_hashes), f, ensure_ascii=False, indent=2)
    
    def _generate_question_hash(self, title: str, url: str) -> str:
        """ç”Ÿæˆé—®é¢˜çš„å”¯ä¸€æ ‡è¯†å“ˆå¸Œ"""
        # æå–é—®é¢˜ID
        question_id = re.search(r'/question/(\d+)', url)
        if question_id:
            return f"q_{question_id.group(1)}"
        else:
            # å¦‚æœæ²¡æœ‰é—®é¢˜IDï¼Œä½¿ç”¨æ ‡é¢˜å“ˆå¸Œ
            return hashlib.md5(title.encode('utf-8')).hexdigest()
    
    def setup_driver(self):
        """è®¾ç½®Chromeé©±åŠ¨"""
        options = Options()
        
        # åŸºæœ¬åæ£€æµ‹è®¾ç½®
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        
        # æ€§èƒ½ä¼˜åŒ–
        options.add_argument('--disable-gpu')
        options.add_argument('--disable-software-rasterizer')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        
        # é™é»˜æ¨¡å¼ï¼ˆå®šæ—¶ä»»åŠ¡æ—¶ä½¿ç”¨ï¼‰
        if hasattr(self, 'headless') and self.headless:
            options.add_argument('--headless')
        
        # User-Agent
        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        self.driver = webdriver.Chrome(options=options)
        self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        self.logger.info("Chromeé©±åŠ¨å¯åŠ¨æˆåŠŸ")
    
    def load_cookies(self) -> bool:
        """åŠ è½½Cookie"""
        if os.path.exists(self.cookie_file):
            try:
                with open(self.cookie_file, 'rb') as f:
                    cookies = pickle.load(f)
                
                # è®¿é—®ä¸»é¡µè®¾ç½®åŸŸ
                self.driver.get("https://www.zhihu.com")
                time.sleep(2)
                
                for cookie in cookies:
                    try:
                        self.driver.add_cookie(cookie)
                    except:
                        pass
                
                self.logger.info("CookieåŠ è½½æˆåŠŸ")
                return True
            except Exception as e:
                self.logger.error(f"CookieåŠ è½½å¤±è´¥: {e}")
        return False
    
    def check_login_status(self) -> bool:
        """æ£€æŸ¥ç™»å½•çŠ¶æ€"""
        try:
            self.driver.get("https://www.zhihu.com/hot")
            time.sleep(3)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•
            current_url = self.driver.current_url
            if "signin" in current_url or "login" in current_url:
                return False
            
            # æ£€æŸ¥é¡µé¢æ˜¯å¦æ­£å¸¸åŠ è½½
            try:
                WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "div, section, a"))
                )
                return True
            except:
                return False
                
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥ç™»å½•çŠ¶æ€å¤±è´¥: {e}")
            return False
    
    def extract_detailed_info(self, url: str) -> Dict:
        """æå–é—®é¢˜è¯¦ç»†ä¿¡æ¯"""
        details = {
            'answer_count': 0,
            'follower_count': 0,
            'view_count': 0,
            'question_tags': [],
            'created_time': None
        }
        
        try:
            # æ‰“å¼€æ–°æ ‡ç­¾é¡µ
            self.driver.execute_script("window.open('');")
            self.driver.switch_to.window(self.driver.window_handles[-1])
            self.driver.get(url)
            time.sleep(2)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # æå–å›ç­”æ•°
            try:
                answer_elem = self.driver.find_element(By.CSS_SELECTOR, 
                    "[class*='NumberBoard-itemValue'], [class*='List-headerText'], .NumberBoard-value")
                answer_text = answer_elem.text
                answer_match = re.search(r'(\d+)', answer_text)
                if answer_match:
                    details['answer_count'] = int(answer_match.group(1))
            except:
                pass
            
            # æå–å…³æ³¨æ•°
            try:
                follower_elems = self.driver.find_elements(By.CSS_SELECTOR, 
                    "[class*='NumberBoard-itemValue'], .NumberBoard-value")
                for elem in follower_elems:
                    text = elem.text
                    if 'å…³æ³¨' in elem.find_element(By.XPATH, "..").text:
                        follower_match = re.search(r'(\d+)', text)
                        if follower_match:
                            details['follower_count'] = int(follower_match.group(1))
                            break
            except:
                pass
            
            # æå–æµè§ˆæ•°ï¼ˆé€šå¸¸åœ¨é—®é¢˜æè¿°é™„è¿‘ï¼‰
            try:
                view_elem = self.driver.find_element(By.CSS_SELECTOR, 
                    "[class*='ContentItem-meta'], [class*='QuestionHeader-detail']")
                view_text = view_elem.text
                view_match = re.search(r'(\d+(?:,\d+)*)\s*æ¬¡æµè§ˆ', view_text)
                if view_match:
                    details['view_count'] = int(view_match.group(1).replace(',', ''))
            except:
                pass
            
            # æå–æ ‡ç­¾
            try:
                tag_elems = self.driver.find_elements(By.CSS_SELECTOR, 
                    ".QuestionHeader-tags .Tag, [class*='QuestionTopic'] .Tag")
                for tag_elem in tag_elems:
                    tag_text = tag_elem.text.strip()
                    if tag_text and tag_text not in details['question_tags']:
                        details['question_tags'].append(tag_text)
            except:
                pass
            
            # å…³é—­å½“å‰æ ‡ç­¾é¡µ
            self.driver.close()
            self.driver.switch_to.window(self.driver.window_handles[0])
            
        except Exception as e:
            self.logger.error(f"æå–è¯¦ç»†ä¿¡æ¯å¤±è´¥ {url}: {e}")
            # ç¡®ä¿åˆ‡æ¢å›ä¸»çª—å£
            try:
                if len(self.driver.window_handles) > 1:
                    self.driver.close()
                    self.driver.switch_to.window(self.driver.window_handles[0])
            except:
                pass
        
        return details
    
    def crawl_hot_list(self, extract_details=True) -> List[Dict]:
        """çˆ¬å–çƒ­æ¦œæ•°æ®ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        self.logger.info("å¼€å§‹çˆ¬å–çƒ­æ¦œæ•°æ®")
        
        if "hot" not in self.driver.current_url:
            self.driver.get("https://www.zhihu.com/hot")
            time.sleep(3)
        
        hot_items = []
        new_items_count = 0
        
        try:
            # ç­‰å¾…é¡µé¢åŠ è½½
            WebDriverWait(self.driver, 20).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # å¤šç§é€‰æ‹©å™¨ç­–ç•¥
            selectors = [
                "div.HotItem",
                "section.HotItem", 
                "[class*='HotItem']",
                "div[data-za-detail-view-id]"
            ]
            
            elements = []
            for selector in selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements:
                        self.logger.info(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
                        break
                except:
                    continue
            
            # å¤‡ç”¨æ–¹æ¡ˆï¼šé€šè¿‡é“¾æ¥æŸ¥æ‰¾
            if not elements:
                self.logger.info("ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆï¼šé€šè¿‡é“¾æ¥æŸ¥æ‰¾")
                all_links = self.driver.find_elements(By.TAG_NAME, "a")
                question_links = [link for link in all_links 
                                if link.get_attribute("href") and "/question/" in link.get_attribute("href")]
                
                for idx, link in enumerate(question_links[:50], 1):
                    try:
                        title = link.text.strip()
                        url = link.get_attribute('href')
                        
                        if not title or not url:
                            continue
                        
                        # å»é‡æ£€æŸ¥
                        question_hash = self._generate_question_hash(title, url)
                        if question_hash in self.question_hashes:
                            continue
                        
                        # åŸºç¡€æ•°æ®
                        item = {
                            'rank': idx,
                            'title': title,
                            'url': url,
                            'question_hash': question_hash,
                            'heat_value': None,
                            'crawl_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                            'date': datetime.now().strftime('%Y-%m-%d')
                        }
                        
                        # æå–è¯¦ç»†ä¿¡æ¯
                        if extract_details and idx <= 20:  # åªå¯¹å‰20æ¡æå–è¯¦ç»†ä¿¡æ¯
                            self.logger.info(f"æ­£åœ¨æå–ç¬¬ {idx} æ¡è¯¦ç»†ä¿¡æ¯...")
                            details = self.extract_detailed_info(url)
                            item.update(details)
                            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
                        
                        hot_items.append(item)
                        self.question_hashes.add(question_hash)
                        new_items_count += 1
                        
                        self.logger.info(f"æå–ç¬¬ {idx} æ¡: {title[:50]}...")
                        
                    except Exception as e:
                        self.logger.error(f"å¤„ç†ç¬¬ {idx} æ¡æ—¶å‡ºé”™: {e}")
                        continue
            else:
                # è§£ææ ‡å‡†çƒ­æ¦œå…ƒç´ 
                for idx, element in enumerate(elements[:50], 1):
                    try:
                        item = self._parse_hot_item_enhanced(element, idx, extract_details and idx <= 20)
                        if item and item.get('title'):
                            # å»é‡æ£€æŸ¥
                            question_hash = item.get('question_hash')
                            if question_hash and question_hash not in self.question_hashes:
                                hot_items.append(item)
                                self.question_hashes.add(question_hash)
                                new_items_count += 1
                                self.logger.info(f"è§£æç¬¬ {idx} æ¡: {item['title'][:50]}...")
                            
                    except Exception as e:
                        self.logger.error(f"è§£æç¬¬ {idx} æ¡æ—¶å‡ºé”™: {e}")
                        
        except Exception as e:
            self.logger.error(f"çˆ¬å–è¿‡ç¨‹å‡ºé”™: {e}")
        
        self.logger.info(f"æœ¬æ¬¡çˆ¬å–å®Œæˆï¼Œæ–°å¢ {new_items_count} æ¡æ•°æ®ï¼Œæ€»è®¡ {len(hot_items)} æ¡")
        return hot_items
    
    def _parse_hot_item_enhanced(self, element, rank: int, extract_details: bool = True) -> Optional[Dict]:
        """è§£æçƒ­æ¦œé¡¹ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        item = {
            'rank': rank,
            'crawl_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'date': datetime.now().strftime('%Y-%m-%d')
        }
        
        try:
            # æå–æ ‡é¢˜å’Œé“¾æ¥
            title_elem = element.find_element(By.CSS_SELECTOR, "h2, [class*='title'], a")
            item['title'] = title_elem.text.strip()
            
            link_elem = element.find_element(By.CSS_SELECTOR, "a[href]")
            item['url'] = link_elem.get_attribute('href')
            
            # ç”Ÿæˆé—®é¢˜å“ˆå¸Œ
            item['question_hash'] = self._generate_question_hash(item['title'], item['url'])
            
            # æå–çƒ­åº¦å€¼
            try:
                heat_elem = element.find_element(By.CSS_SELECTOR, 
                    "[class*='metrics'], [class*='hot'], [class*='HotItem-metrics']")
                item['heat_value'] = heat_elem.text.strip()
            except:
                item['heat_value'] = None
            
            # æå–è¯¦ç»†ä¿¡æ¯
            if extract_details and item.get('url'):
                details = self.extract_detailed_info(item['url'])
                item.update(details)
                time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
                
        except Exception as e:
            self.logger.error(f"è§£æçƒ­æ¦œé¡¹å¤±è´¥: {e}")
            return None
            
        return item if 'title' in item else None
    
    def save_data(self, data: List[Dict], filename_prefix: str = "zhihu_hot"):
        """ä¿å­˜æ•°æ®"""
        if not data:
            self.logger.warning("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return None
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜åŸå§‹æ•°æ®
        raw_filename = f"{filename_prefix}_{timestamp}.json"
        raw_filepath = os.path.join(self.data_dir, "raw", raw_filename)
        
        with open(raw_filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜å»é‡å“ˆå¸Œ
        self._save_question_hashes()
        
        # æ›´æ–°çˆ¬å–å†å²
        self._update_crawl_history(len(data), raw_filepath)
        
        self.logger.info(f"æ•°æ®å·²ä¿å­˜åˆ°: {raw_filepath}")
        return raw_filepath
    
    def _update_crawl_history(self, count: int, filepath: str):
        """æ›´æ–°çˆ¬å–å†å²è®°å½•"""
        history = []
        if os.path.exists(self.history_file):
            try:
                with open(self.history_file, 'r', encoding='utf-8') as f:
                    history = json.load(f)
            except:
                pass
        
        history.append({
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'count': count,
            'filepath': filepath
        })
        
        # åªä¿ç•™æœ€è¿‘100æ¡è®°å½•
        history = history[-100:]
        
        with open(self.history_file, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
    
    def run_single_crawl(self, extract_details: bool = True, headless: bool = False) -> Optional[str]:
        """æ‰§è¡Œå•æ¬¡çˆ¬å–"""
        self.headless = headless
        
        try:
            self.setup_driver()
            
            # åŠ è½½Cookieå¹¶æ£€æŸ¥ç™»å½•çŠ¶æ€
            if self.load_cookies():
                self.driver.refresh()
                time.sleep(3)
                
                if not self.check_login_status():
                    self.logger.error("ç™»å½•çŠ¶æ€æ£€æŸ¥å¤±è´¥ï¼Œè¯·é‡æ–°ç™»å½•")
                    return None
            else:
                self.logger.error("æ— æ³•åŠ è½½Cookieï¼Œè¯·å…ˆæ‰‹åŠ¨ç™»å½•")
                return None
            
            # æ‰§è¡Œçˆ¬å–
            hot_items = self.crawl_hot_list(extract_details)
            
            if hot_items:
                filepath = self.save_data(hot_items)
                return filepath
            else:
                self.logger.warning("æœªèƒ½çˆ¬å–åˆ°æ•°æ®")
                return None
                
        except Exception as e:
            self.logger.error(f"çˆ¬å–è¿‡ç¨‹å‡ºé”™: {e}")
            return None
        finally:
            if self.driver:
                self.driver.quit()


class HotListAnalyzer:
    """çƒ­æ¦œæ•°æ®åˆ†æå™¨"""
    
    def __init__(self, data_dir: str = "data"):
        self.data_dir = data_dir
        self.raw_dir = os.path.join(data_dir, "raw")
        self.analysis_dir = os.path.join(data_dir, "analysis")
        self.reports_dir = os.path.join(data_dir, "reports")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        for directory in [self.analysis_dir, self.reports_dir]:
            os.makedirs(directory, exist_ok=True)
    
    def load_all_data(self) -> pd.DataFrame:
        """åŠ è½½æ‰€æœ‰å†å²æ•°æ®"""
        all_data = []
        
        if not os.path.exists(self.raw_dir):
            return pd.DataFrame()
        
        for filename in os.listdir(self.raw_dir):
            if filename.endswith('.json') and 'zhihu_hot' in filename:
                filepath = os.path.join(self.raw_dir, filename)
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        all_data.extend(data)
                except Exception as e:
                    print(f"åŠ è½½æ–‡ä»¶å¤±è´¥ {filename}: {e}")
        
        if not all_data:
            return pd.DataFrame()
        
        df = pd.DataFrame(all_data)
        
        # æ•°æ®ç±»å‹è½¬æ¢
        if 'crawl_time' in df.columns:
            df['crawl_time'] = pd.to_datetime(df['crawl_time'])
        if 'date' in df.columns:
            df['date'] = pd.to_datetime(df['date'])
        
        return df
    
    def analyze_hot_trends(self, days: int = 7) -> Dict:
        """åˆ†æçƒ­åº¦è¶‹åŠ¿"""
        df = self.load_all_data()
        
        if df.empty:
            return {"error": "æ²¡æœ‰å¯åˆ†æçš„æ•°æ®"}
        
        # è¿‡æ»¤æœ€è¿‘Nå¤©çš„æ•°æ®
        cutoff_date = datetime.now() - timedelta(days=days)
        if 'crawl_time' in df.columns:
            recent_df = df[df['crawl_time'] >= cutoff_date]
        else:
            recent_df = df
        
        analysis = {
            'total_questions': len(recent_df['question_hash'].unique()) if 'question_hash' in recent_df.columns else len(recent_df),
            'total_records': len(recent_df),
            'date_range': {
                'start': recent_df['crawl_time'].min().strftime('%Y-%m-%d') if 'crawl_time' in recent_df.columns else 'N/A',
                'end': recent_df['crawl_time'].max().strftime('%Y-%m-%d') if 'crawl_time' in recent_df.columns else 'N/A'
            }
        }
        
        # æ¯æ—¥é—®é¢˜æ•°é‡è¶‹åŠ¿
        if 'date' in recent_df.columns:
            daily_counts = recent_df.groupby('date').size()
            analysis['daily_trends'] = daily_counts.to_dict()
        
        # çƒ­é—¨æ ‡ç­¾åˆ†æ
        if 'question_tags' in recent_df.columns:
            all_tags = []
            for tags in recent_df['question_tags'].dropna():
                if isinstance(tags, list):
                    all_tags.extend(tags)
            
            if all_tags:
                tag_counts = pd.Series(all_tags).value_counts().head(10)
                analysis['popular_tags'] = tag_counts.to_dict()
        
        # å›ç­”æ•°åˆ†æ
        if 'answer_count' in recent_df.columns:
            answer_stats = recent_df['answer_count'].describe()
            analysis['answer_stats'] = {
                'mean': round(answer_stats['mean'], 2),
                'median': answer_stats['50%'],
                'max': answer_stats['max'],
                'min': answer_stats['min']
            }
        
        # æ’åç¨³å®šæ€§åˆ†æï¼ˆåŒä¸€é—®é¢˜åœ¨ä¸åŒæ—¶é—´çš„æ’åå˜åŒ–ï¼‰
        if 'question_hash' in recent_df.columns and 'rank' in recent_df.columns:
            rank_stability = recent_df.groupby('question_hash')['rank'].agg(['count', 'mean', 'std']).reset_index()
            rank_stability = rank_stability[rank_stability['count'] > 1]  # åªåˆ†æå‡ºç°å¤šæ¬¡çš„é—®é¢˜
            
            if not rank_stability.empty:
                analysis['rank_stability'] = {
                    'stable_questions': len(rank_stability[rank_stability['std'] < 5]),  # æ’åå˜åŒ–å°äº5
                    'volatile_questions': len(rank_stability[rank_stability['std'] >= 10]),  # æ’åå˜åŒ–å¤§äº10
                    'avg_rank_change': round(rank_stability['std'].mean(), 2)
                }
        
        return analysis
    
    def generate_trend_charts(self, days: int = 7):
        """ç”Ÿæˆè¶‹åŠ¿å›¾è¡¨"""
        df = self.load_all_data()
        
        if df.empty:
            print("æ²¡æœ‰æ•°æ®å¯ç”¨äºç”Ÿæˆå›¾è¡¨")
            return
        
        # è®¾ç½®å›¾è¡¨æ ·å¼
        plt.style.use('seaborn-v0_8')
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle(f'çŸ¥ä¹çƒ­æ¦œæ•°æ®åˆ†æ - æœ€è¿‘{days}å¤©', fontsize=16, fontweight='bold')
        
        # è¿‡æ»¤æ•°æ®
        cutoff_date = datetime.now() - timedelta(days=days)
        if 'crawl_time' in df.columns:
            recent_df = df[df['crawl_time'] >= cutoff_date]
        else:
            recent_df = df
        
        # 1. æ¯æ—¥çˆ¬å–æ•°é‡è¶‹åŠ¿
        if 'date' in recent_df.columns:
            daily_counts = recent_df.groupby('date').size()
            daily_counts.plot(kind='line', ax=axes[0,0], marker='o', color='#1f77b4')
            axes[0,0].set_title('æ¯æ—¥çˆ¬å–æ•°é‡è¶‹åŠ¿')
            axes[0,0].set_xlabel('æ—¥æœŸ')
            axes[0,0].set_ylabel('é—®é¢˜æ•°é‡')
            axes[0,0].grid(True, alpha=0.3)
        
        # 2. æ’ååˆ†å¸ƒ
        if 'rank' in recent_df.columns:
            rank_counts = recent_df['rank'].value_counts().head(20)
            rank_counts.plot(kind='bar', ax=axes[0,1], color='#ff7f0e')
            axes[0,1].set_title('æ’ååˆ†å¸ƒï¼ˆTop 20ï¼‰')
            axes[0,1].set_xlabel('æ’å')
            axes[0,1].set_ylabel('å‡ºç°æ¬¡æ•°')
        
        # 3. å›ç­”æ•°åˆ†å¸ƒ
        if 'answer_count' in recent_df.columns and recent_df['answer_count'].sum() > 0:
            answer_counts = recent_df['answer_count'].dropna()
            answer_counts.hist(bins=20, ax=axes[1,0], color='#2ca02c', alpha=0.7)
            axes[1,0].set_title('å›ç­”æ•°åˆ†å¸ƒ')
            axes[1,0].set_xlabel('å›ç­”æ•°')
            axes[1,0].set_ylabel('é—®é¢˜æ•°é‡')
        
        # 4. çƒ­é—¨æ ‡ç­¾
        if 'question_tags' in recent_df.columns:
            all_tags = []
            for tags in recent_df['question_tags'].dropna():
                if isinstance(tags, list):
                    all_tags.extend(tags)
            
            if all_tags:
                tag_counts = pd.Series(all_tags).value_counts().head(10)
                tag_counts.plot(kind='barh', ax=axes[1,1], color='#d62728')
                axes[1,1].set_title('çƒ­é—¨æ ‡ç­¾ï¼ˆTop 10ï¼‰')
                axes[1,1].set_xlabel('å‡ºç°æ¬¡æ•°')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        chart_path = os.path.join(self.analysis_dir, f'trend_analysis_{datetime.now().strftime("%Y%m%d_%H%M%S")}.png')
        plt.savefig(chart_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"å›¾è¡¨å·²ä¿å­˜åˆ°: {chart_path}")
    
    def generate_report(self, days: int = 7) -> str:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        analysis = self.analyze_hot_trends(days)
        
        if "error" in analysis:
            return analysis["error"]
        
        report_content = f"""
# çŸ¥ä¹çƒ­æ¦œæ•°æ®åˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**åˆ†æå‘¨æœŸ**: æœ€è¿‘{days}å¤©

## ğŸ“Š æ•°æ®æ¦‚è§ˆ

- **æ€»é—®é¢˜æ•°**: {analysis.get('total_questions', 'N/A')} ä¸ªç‹¬ç‰¹é—®é¢˜
- **æ€»è®°å½•æ•°**: {analysis.get('total_records', 'N/A')} æ¡è®°å½•
- **æ•°æ®æ—¶é—´èŒƒå›´**: {analysis.get('date_range', {}).get('start', 'N/A')} è‡³ {analysis.get('date_range', {}).get('end', 'N/A')}

## ğŸ“ˆ è¶‹åŠ¿åˆ†æ

### æ¯æ—¥æ´»è·ƒåº¦
"""
        
        if 'daily_trends' in analysis:
            report_content += "\n| æ—¥æœŸ | é—®é¢˜æ•°é‡ |\n|------|----------|\n"
            for date, count in analysis['daily_trends'].items():
                report_content += f"| {date} | {count} |\n"
        
        if 'popular_tags' in analysis:
            report_content += "\n\n### ğŸ·ï¸ çƒ­é—¨æ ‡ç­¾\n\n"
            for tag, count in analysis['popular_tags'].items():
                report_content += f"- **{tag}**: {count} æ¬¡\n"
        
        if 'answer_stats' in analysis:
            stats = analysis['answer_stats']
            report_content += f"""

### ğŸ“ å›ç­”æ•°æ®ç»Ÿè®¡

- **å¹³å‡å›ç­”æ•°**: {stats['mean']} ä¸ª
- **ä¸­ä½æ•°å›ç­”æ•°**: {stats['median']} ä¸ª  
- **æœ€å¤šå›ç­”æ•°**: {stats['max']} ä¸ª
- **æœ€å°‘å›ç­”æ•°**: {stats['min']} ä¸ª
"""
        
        if 'rank_stability' in analysis:
            stability = analysis['rank_stability']
            report_content += f"""

### ğŸ“Š æ’åç¨³å®šæ€§åˆ†æ

- **ç¨³å®šé—®é¢˜æ•°** (æ’åå˜åŒ–<5): {stability['stable_questions']} ä¸ª
- **æ³¢åŠ¨é—®é¢˜æ•°** (æ’åå˜åŒ–â‰¥10): {stability['volatile_questions']} ä¸ª
- **å¹³å‡æ’åå˜åŒ–**: {stability['avg_rank_change']}
"""
        
        # ä¿å­˜æŠ¥å‘Š
        report_filename = f"analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        report_path = os.path.join(self.reports_dir, report_filename)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        return report_path


class ScheduledCrawler:
    """å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨"""
    
    def __init__(self, data_dir: str = "data"):
        self.crawler = EnhancedZhihuCrawler(data_dir)
        self.analyzer = HotListAnalyzer(data_dir)
        self.scheduler = BlockingScheduler()
        
        # é…ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
    
    def scheduled_crawl_job(self, extract_details: bool = True):
        """å®šæ—¶çˆ¬å–ä»»åŠ¡"""
        self.logger.info("å¼€å§‹æ‰§è¡Œå®šæ—¶çˆ¬å–ä»»åŠ¡")
        
        try:
            filepath = self.crawler.run_single_crawl(extract_details=extract_details, headless=True)
            if filepath:
                self.logger.info(f"å®šæ—¶çˆ¬å–å®Œæˆï¼Œæ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
                
                # æ¯å¤©æ™šä¸Šç”Ÿæˆåˆ†ææŠ¥å‘Š
                current_hour = datetime.now().hour
                if current_hour == 23:  # æ™šä¸Š11ç‚¹ç”ŸæˆæŠ¥å‘Š
                    self.logger.info("ç”Ÿæˆæ¯æ—¥åˆ†ææŠ¥å‘Š")
                    self.analyzer.generate_report(days=1)
                    
            else:
                self.logger.error("å®šæ—¶çˆ¬å–å¤±è´¥")
                
        except Exception as e:
            self.logger.error(f"å®šæ—¶ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
    
    def scheduled_analysis_job(self, days: int = 7):
        """å®šæ—¶åˆ†æä»»åŠ¡"""
        self.logger.info("å¼€å§‹æ‰§è¡Œå®šæ—¶åˆ†æä»»åŠ¡")
        
        try:
            # ç”Ÿæˆåˆ†ææŠ¥å‘Š
            report_path = self.analyzer.generate_report(days=days)
            self.logger.info(f"åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
            
            # ç”Ÿæˆå›¾è¡¨
            self.analyzer.generate_trend_charts(days=days)
            self.logger.info("è¶‹åŠ¿å›¾è¡¨å·²ç”Ÿæˆ")
            
        except Exception as e:
            self.logger.error(f"åˆ†æä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
    
    def add_crawl_jobs(self):
        """æ·»åŠ çˆ¬å–ä»»åŠ¡"""
        # æ¯2å°æ—¶çˆ¬å–ä¸€æ¬¡ï¼ˆåŸºç¡€ä¿¡æ¯ï¼‰
        self.scheduler.add_job(
            func=self.scheduled_crawl_job,
            trigger=CronTrigger(minute=0, second=0, hour='*/2'),
            args=[False],  # ä¸æå–è¯¦ç»†ä¿¡æ¯ï¼Œæé«˜é€Ÿåº¦
            id='crawl_basic_2h',
            name='æ¯2å°æ—¶åŸºç¡€çˆ¬å–',
            misfire_grace_time=300
        )
        
        # æ¯6å°æ—¶è¯¦ç»†çˆ¬å–ä¸€æ¬¡ï¼ˆåŒ…å«è¯¦ç»†ä¿¡æ¯ï¼‰
        self.scheduler.add_job(
            func=self.scheduled_crawl_job,
            trigger=CronTrigger(minute=30, second=0, hour='*/6'),
            args=[True],  # æå–è¯¦ç»†ä¿¡æ¯
            id='crawl_detailed_6h',
            name='æ¯6å°æ—¶è¯¦ç»†çˆ¬å–',
            misfire_grace_time=600
        )
        
        self.logger.info("çˆ¬å–ä»»åŠ¡å·²æ·»åŠ ")
    
    def add_analysis_jobs(self):
        """æ·»åŠ åˆ†æä»»åŠ¡"""
        # æ¯å¤©æ—©ä¸Š8ç‚¹ç”Ÿæˆå‘¨æŠ¥å‘Š
        self.scheduler.add_job(
            func=self.scheduled_analysis_job,
            trigger=CronTrigger(hour=8, minute=0, second=0),
            args=[7],  # 7å¤©æŠ¥å‘Š
            id='weekly_analysis',
            name='æ¯æ—¥å‘¨æŠ¥å‘Šç”Ÿæˆ',
            misfire_grace_time=3600
        )
        
        # æ¯å‘¨ä¸€æ—©ä¸Š9ç‚¹ç”ŸæˆæœˆæŠ¥å‘Š
        self.scheduler.add_job(
            func=self.scheduled_analysis_job,
            trigger=CronTrigger(day_of_week='mon', hour=9, minute=0, second=0),
            args=[30],  # 30å¤©æŠ¥å‘Š
            id='monthly_analysis',
            name='æ¯å‘¨æœˆæŠ¥å‘Šç”Ÿæˆ',
            misfire_grace_time=3600
        )
        
        self.logger.info("åˆ†æä»»åŠ¡å·²æ·»åŠ ")
    
    def start_scheduler(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        self.add_crawl_jobs()
        self.add_analysis_jobs()
        
        self.logger.info("å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å¯åŠ¨")
        self.logger.info("å·²æ·»åŠ çš„ä»»åŠ¡:")
        for job in self.scheduler.get_jobs():
            self.logger.info(f"  - {job.name} (ID: {job.id})")
        
        try:
            self.scheduler.start()
        except KeyboardInterrupt:
            self.logger.info("æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å…³é—­è°ƒåº¦å™¨...")
            self.scheduler.shutdown()
    
    def run_manual_analysis(self, days: int = 7):
        """æ‰‹åŠ¨è¿è¡Œåˆ†æ"""
        self.logger.info(f"å¼€å§‹æ‰‹åŠ¨åˆ†ææœ€è¿‘{days}å¤©çš„æ•°æ®")
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        report_path = self.analyzer.generate_report(days=days)
        print(f"\nâœ“ åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        
        # ç”Ÿæˆè¶‹åŠ¿å›¾è¡¨
        self.analyzer.generate_trend_charts(days=days)
        print("âœ“ è¶‹åŠ¿å›¾è¡¨å·²ç”Ÿæˆ")
        
        # æ˜¾ç¤ºå…³é”®ç»Ÿè®¡ä¿¡æ¯
        analysis = self.analyzer.analyze_hot_trends(days=days)
        if "error" not in analysis:
            print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡ (æœ€è¿‘{days}å¤©):")
            print(f"  - æ€»é—®é¢˜æ•°: {analysis.get('total_questions', 'N/A')}")
            print(f"  - æ€»è®°å½•æ•°: {analysis.get('total_records', 'N/A')}")
            
            if 'popular_tags' in analysis:
                print("  - çƒ­é—¨æ ‡ç­¾:")
                for tag, count in list(analysis['popular_tags'].items())[:5]:
                    print(f"    * {tag}: {count}æ¬¡")


def main():
    """ä¸»å‡½æ•° - å¢å¼ºç‰ˆåŠŸèƒ½æ¼”ç¤º"""
    print("="*60)
    print("çŸ¥ä¹çƒ­æ¦œçˆ¬è™«ç³»ç»Ÿ - å¢å¼ºç‰ˆ")
    print("="*60)
    print("åŠŸèƒ½:")
    print("1. å•æ¬¡çˆ¬å– (åŸºç¡€)")
    print("2. å•æ¬¡çˆ¬å– (è¯¦ç»†ä¿¡æ¯)")
    print("3. æ•°æ®åˆ†æ")
    print("4. å¯åŠ¨å®šæ—¶ä»»åŠ¡")
    print("5. ç”Ÿæˆè¶‹åŠ¿å›¾è¡¨")
    print("6. é€€å‡º")
    print("="*60)
    
    crawler = EnhancedZhihuCrawler()
    analyzer = HotListAnalyzer()
    scheduler = ScheduledCrawler()
    
    while True:
        try:
            choice = input("\nè¯·é€‰æ‹©åŠŸèƒ½ (1-6): ").strip()
            
            if choice == '1':
                print("\nå¼€å§‹åŸºç¡€çˆ¬å–...")
                filepath = crawler.run_single_crawl(extract_details=False)
                if filepath:
                    print(f"âœ“ çˆ¬å–å®Œæˆï¼Œæ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
                else:
                    print("âœ— çˆ¬å–å¤±è´¥")
            
            elif choice == '2':
                print("\nå¼€å§‹è¯¦ç»†çˆ¬å–ï¼ˆåŒ…å«å›ç­”æ•°ã€æµè§ˆæ•°ç­‰ï¼‰...")
                filepath = crawler.run_single_crawl(extract_details=True)
                if filepath:
                    print(f"âœ“ çˆ¬å–å®Œæˆï¼Œæ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
                else:
                    print("âœ— çˆ¬å–å¤±è´¥")
            
            elif choice == '3':
                days = input("è¯·è¾“å…¥åˆ†æå¤©æ•° (é»˜è®¤7å¤©): ").strip()
                days = int(days) if days.isdigit() else 7
                scheduler.run_manual_analysis(days)
            
            elif choice == '4':
                print("\nå¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨...")
                print("ä»»åŠ¡é…ç½®:")
                print("  - æ¯2å°æ—¶åŸºç¡€çˆ¬å–")
                print("  - æ¯6å°æ—¶è¯¦ç»†çˆ¬å–") 
                print("  - æ¯å¤©8ç‚¹ç”Ÿæˆå‘¨æŠ¥å‘Š")
                print("  - æ¯å‘¨ä¸€9ç‚¹ç”ŸæˆæœˆæŠ¥å‘Š")
                print("\næŒ‰ Ctrl+C åœæ­¢è°ƒåº¦å™¨")
                scheduler.start_scheduler()
            
            elif choice == '5':
                days = input("è¯·è¾“å…¥åˆ†æå¤©æ•° (é»˜è®¤7å¤©): ").strip()
                days = int(days) if days.isdigit() else 7
                analyzer.generate_trend_charts(days)
            
            elif choice == '6':
                print("ç¨‹åºé€€å‡º")
                break
            
            else:
                print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
                
        except KeyboardInterrupt:
            print("\n\nç¨‹åºè¢«ä¸­æ–­")
            break
        except Exception as e:
            print(f"å‘ç”Ÿé”™è¯¯: {e}")


if __name__ == "__main__":
    main()