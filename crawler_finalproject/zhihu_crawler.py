#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
çŸ¥ä¹çƒ­æ¦œçˆ¬è™«ç³»ç»Ÿ - å¢å¼ºç‰ˆ
åŠŸèƒ½ï¼šå®šæ—¶ä»»åŠ¡ã€æ•°æ®åˆ†æã€å»é‡ã€æ‰©å±•å­—æ®µé‡‡é›†
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from apscheduler.schedulers.blocking import BlockingScheduler
from apscheduler.triggers.cron import CronTrigger
import time
import json
import os
import numpy as np
import pickle
import hashlib
import re
from datetime import datetime, timedelta
from collections import defaultdict
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Optional, Tuple
import logging

# é…ç½®matplotlibä¸­æ–‡å­—ä½“
# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œæ ·å¼
plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.dpi'] = 100
plt.rcParams['savefig.dpi'] = 300

class EnhancedZhihuCrawler:
    """å¢å¼ºç‰ˆçŸ¥ä¹çˆ¬è™« - æ”¯æŒå®šæ—¶ä»»åŠ¡ã€æ•°æ®åˆ†æã€å»é‡ç­‰åŠŸèƒ½"""
    
    def __init__(self, data_dir="data"):
        self.data_dir = data_dir
        self.cookie_file = os.path.join(data_dir, "zhihu_cookies.pkl")
        self.history_file = os.path.join(data_dir, "crawl_history.json")
        self.duplicate_file = os.path.join(data_dir, "question_hashes.json")
        
        # åˆ›å»ºå¿…è¦ç›®å½•
        for subdir in ["raw", "processed", "analysis", "reports"]:
            os.makedirs(os.path.join(data_dir, subdir), exist_ok=True)
        
        # é…ç½®æ—¥å¿—
        self._setup_logging()
        
        # åˆå§‹åŒ–å»é‡æ•°æ®
        self.question_hashes = self._load_question_hashes()
        
        self.driver = None
        
    def _setup_logging(self):
        """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_dir = os.path.join(self.data_dir, "logs")
        os.makedirs(log_dir, exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(os.path.join(log_dir, f"crawler_{datetime.now().strftime('%Y%m%d')}.log")),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def _load_question_hashes(self) -> set:
        """åŠ è½½å·²çŸ¥é—®é¢˜çš„å“ˆå¸Œå€¼ç”¨äºå»é‡"""
        if os.path.exists(self.duplicate_file):
            try:
                with open(self.duplicate_file, 'r', encoding='utf-8') as f:
                    return set(json.load(f))
            except:
                return set()
        return set()
    
    def _save_question_hashes(self):
        """ä¿å­˜é—®é¢˜å“ˆå¸Œå€¼"""
        with open(self.duplicate_file, 'w', encoding='utf-8') as f:
            json.dump(list(self.question_hashes), f, ensure_ascii=False, indent=2)
    
    def _generate_question_hash(self, title: str, url: str) -> str:
        """ç”Ÿæˆé—®é¢˜çš„å”¯ä¸€æ ‡è¯†å“ˆå¸Œ"""
        # æå–é—®é¢˜ID
        question_id = re.search(r'/question/(\d+)', url)
        if question_id:
            return f"q_{question_id.group(1)}"
        else:
            # å¦‚æœæ²¡æœ‰é—®é¢˜IDï¼Œä½¿ç”¨æ ‡é¢˜å“ˆå¸Œ
            return hashlib.md5(title.encode('utf-8')).hexdigest()
    
    def setup_driver(self):
        """è®¾ç½®Chromeé©±åŠ¨"""
        options = Options()
        
        # åŸºæœ¬åæ£€æµ‹è®¾ç½®
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        
        # æ€§èƒ½ä¼˜åŒ–
        options.add_argument('--disable-gpu')
        options.add_argument('--disable-software-rasterizer')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        
        # é™é»˜æ¨¡å¼ï¼ˆå®šæ—¶ä»»åŠ¡æ—¶ä½¿ç”¨ï¼‰
        if hasattr(self, 'headless') and self.headless:
            options.add_argument('--headless')
        
        # User-Agent
        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        self.driver = webdriver.Chrome(options=options)
        self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        self.logger.info("Chromeé©±åŠ¨å¯åŠ¨æˆåŠŸ")
    
    def load_cookies(self) -> bool:
        """åŠ è½½Cookie"""
        if os.path.exists(self.cookie_file):
            try:
                with open(self.cookie_file, 'rb') as f:
                    cookies = pickle.load(f)
                
                # è®¿é—®ä¸»é¡µè®¾ç½®åŸŸ
                self.driver.get("https://www.zhihu.com")
                time.sleep(2)
                
                for cookie in cookies:
                    try:
                        self.driver.add_cookie(cookie)
                    except:
                        pass
                
                self.logger.info("CookieåŠ è½½æˆåŠŸ")
                return True
            except Exception as e:
                self.logger.error(f"CookieåŠ è½½å¤±è´¥: {e}")
        return False
    
    def check_login_status(self) -> bool:
        """æ£€æŸ¥ç™»å½•çŠ¶æ€"""
        try:
            self.driver.get("https://www.zhihu.com/hot")
            time.sleep(3)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•
            current_url = self.driver.current_url
            if "signin" in current_url or "login" in current_url:
                return False
            
            # æ£€æŸ¥é¡µé¢æ˜¯å¦æ­£å¸¸åŠ è½½
            try:
                WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "div, section, a"))
                )
                return True
            except:
                return False
                
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥ç™»å½•çŠ¶æ€å¤±è´¥: {e}")
            return False
    
    def extract_detailed_info(self, url: str) -> Dict:
        """æå–é—®é¢˜è¯¦ç»†ä¿¡æ¯"""
        details = {
            'answer_count': 0,
            'follower_count': 0,
            'view_count': 0,
            'question_tags': [],
            'created_time': None
        }
        
        try:
            # æ‰“å¼€æ–°æ ‡ç­¾é¡µ
            self.driver.execute_script("window.open('');")
            self.driver.switch_to.window(self.driver.window_handles[-1])
            self.driver.get(url)
            time.sleep(2)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # æå–å›ç­”æ•°
            try:
                answer_elem = self.driver.find_element(By.CSS_SELECTOR, 
                    "[class*='NumberBoard-itemValue'], [class*='List-headerText'], .NumberBoard-value")
                answer_text = answer_elem.text
                answer_match = re.search(r'(\d+)', answer_text)
                if answer_match:
                    details['answer_count'] = int(answer_match.group(1))
            except:
                pass
            
            # æå–å…³æ³¨æ•°
            try:
                follower_elems = self.driver.find_elements(By.CSS_SELECTOR, 
                    "[class*='NumberBoard-itemValue'], .NumberBoard-value")
                for elem in follower_elems:
                    text = elem.text
                    if 'å…³æ³¨' in elem.find_element(By.XPATH, "..").text:
                        follower_match = re.search(r'(\d+)', text)
                        if follower_match:
                            details['follower_count'] = int(follower_match.group(1))
                            break
            except:
                pass
            
            # æå–æµè§ˆæ•°ï¼ˆé€šå¸¸åœ¨é—®é¢˜æè¿°é™„è¿‘ï¼‰
            try:
                view_elem = self.driver.find_element(By.CSS_SELECTOR, 
                    "[class*='ContentItem-meta'], [class*='QuestionHeader-detail']")
                view_text = view_elem.text
                view_match = re.search(r'(\d+(?:,\d+)*)\s*æ¬¡æµè§ˆ', view_text)
                if view_match:
                    details['view_count'] = int(view_match.group(1).replace(',', ''))
            except:
                pass
            
            # æå–æ ‡ç­¾
            try:
                tag_elems = self.driver.find_elements(By.CSS_SELECTOR, 
                    ".QuestionHeader-tags .Tag, [class*='QuestionTopic'] .Tag")
                for tag_elem in tag_elems:
                    tag_text = tag_elem.text.strip()
                    if tag_text and tag_text not in details['question_tags']:
                        details['question_tags'].append(tag_text)
            except:
                pass
            
            # å…³é—­å½“å‰æ ‡ç­¾é¡µ
            self.driver.close()
            self.driver.switch_to.window(self.driver.window_handles[0])
            
        except Exception as e:
            self.logger.error(f"æå–è¯¦ç»†ä¿¡æ¯å¤±è´¥ {url}: {e}")
            # ç¡®ä¿åˆ‡æ¢å›ä¸»çª—å£
            try:
                if len(self.driver.window_handles) > 1:
                    self.driver.close()
                    self.driver.switch_to.window(self.driver.window_handles[0])
            except:
                pass
        
        return details
    
    def crawl_hot_list(self, extract_details=True) -> List[Dict]:
        """çˆ¬å–çƒ­æ¦œæ•°æ®ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        self.logger.info("å¼€å§‹çˆ¬å–çƒ­æ¦œæ•°æ®")
        
        if "hot" not in self.driver.current_url:
            self.driver.get("https://www.zhihu.com/hot")
            time.sleep(3)
        
        hot_items = []
        new_items_count = 0
        
        try:
            # ç­‰å¾…é¡µé¢åŠ è½½
            WebDriverWait(self.driver, 20).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # å¤šç§é€‰æ‹©å™¨ç­–ç•¥
            selectors = [
                "div.HotItem",
                "section.HotItem", 
                "[class*='HotItem']",
                "div[data-za-detail-view-id]"
            ]
            
            elements = []
            for selector in selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements:
                        self.logger.info(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
                        break
                except:
                    continue
            
            # å¤‡ç”¨æ–¹æ¡ˆï¼šé€šè¿‡é“¾æ¥æŸ¥æ‰¾
            if not elements:
                self.logger.info("ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆï¼šé€šè¿‡é“¾æ¥æŸ¥æ‰¾")
                all_links = self.driver.find_elements(By.TAG_NAME, "a")
                question_links = [link for link in all_links 
                                if link.get_attribute("href") and "/question/" in link.get_attribute("href")]
                
                for idx, link in enumerate(question_links[:50], 1):
                    try:
                        title = link.text.strip()
                        url = link.get_attribute('href')
                        
                        if not title or not url:
                            continue
                        
                        # å»é‡æ£€æŸ¥
                        question_hash = self._generate_question_hash(title, url)
                        if question_hash in self.question_hashes:
                            continue
                        
                        # åŸºç¡€æ•°æ®
                        item = {
                            'rank': idx,
                            'title': title,
                            'url': url,
                            'question_hash': question_hash,
                            'heat_value': None,
                            'crawl_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                            'date': datetime.now().strftime('%Y-%m-%d')
                        }
                        
                        # æå–è¯¦ç»†ä¿¡æ¯
                        if extract_details and idx <= 20:  # åªå¯¹å‰20æ¡æå–è¯¦ç»†ä¿¡æ¯
                            self.logger.info(f"æ­£åœ¨æå–ç¬¬ {idx} æ¡è¯¦ç»†ä¿¡æ¯...")
                            details = self.extract_detailed_info(url)
                            item.update(details)
                            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
                        
                        hot_items.append(item)
                        self.question_hashes.add(question_hash)
                        new_items_count += 1
                        
                        self.logger.info(f"æå–ç¬¬ {idx} æ¡: {title[:50]}...")
                        
                    except Exception as e:
                        self.logger.error(f"å¤„ç†ç¬¬ {idx} æ¡æ—¶å‡ºé”™: {e}")
                        continue
            else:
                # è§£ææ ‡å‡†çƒ­æ¦œå…ƒç´ 
                for idx, element in enumerate(elements[:50], 1):
                    try:
                        item = self._parse_hot_item_enhanced(element, idx, extract_details and idx <= 20)
                        if item and item.get('title'):
                            # å»é‡æ£€æŸ¥
                            question_hash = item.get('question_hash')
                            if question_hash and question_hash not in self.question_hashes:
                                hot_items.append(item)
                                self.question_hashes.add(question_hash)
                                new_items_count += 1
                                self.logger.info(f"è§£æç¬¬ {idx} æ¡: {item['title'][:50]}...")
                            
                    except Exception as e:
                        self.logger.error(f"è§£æç¬¬ {idx} æ¡æ—¶å‡ºé”™: {e}")
                        
        except Exception as e:
            self.logger.error(f"çˆ¬å–è¿‡ç¨‹å‡ºé”™: {e}")
        
        self.logger.info(f"æœ¬æ¬¡çˆ¬å–å®Œæˆï¼Œæ–°å¢ {new_items_count} æ¡æ•°æ®ï¼Œæ€»è®¡ {len(hot_items)} æ¡")
        return hot_items
    
    def _parse_hot_item_enhanced(self, element, rank: int, extract_details: bool = True) -> Optional[Dict]:
        """è§£æçƒ­æ¦œé¡¹ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        item = {
            'rank': rank,
            'crawl_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'date': datetime.now().strftime('%Y-%m-%d')
        }
        
        try:
            # æå–æ ‡é¢˜å’Œé“¾æ¥
            title_elem = element.find_element(By.CSS_SELECTOR, "h2, [class*='title'], a")
            item['title'] = title_elem.text.strip()
            
            link_elem = element.find_element(By.CSS_SELECTOR, "a[href]")
            item['url'] = link_elem.get_attribute('href')
            
            # ç”Ÿæˆé—®é¢˜å“ˆå¸Œ
            item['question_hash'] = self._generate_question_hash(item['title'], item['url'])
            
            # æå–çƒ­åº¦å€¼
            try:
                heat_elem = element.find_element(By.CSS_SELECTOR, 
                    "[class*='metrics'], [class*='hot'], [class*='HotItem-metrics']")
                item['heat_value'] = heat_elem.text.strip()
            except:
                item['heat_value'] = None
            
            # æå–è¯¦ç»†ä¿¡æ¯
            if extract_details and item.get('url'):
                details = self.extract_detailed_info(item['url'])
                item.update(details)
                time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
                
        except Exception as e:
            self.logger.error(f"è§£æçƒ­æ¦œé¡¹å¤±è´¥: {e}")
            return None
            
        return item if 'title' in item else None
    
    def save_data(self, data: List[Dict], filename_prefix: str = "zhihu_hot"):
        """ä¿å­˜æ•°æ®"""
        if not data:
            self.logger.warning("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return None
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜åŸå§‹æ•°æ®
        raw_filename = f"{filename_prefix}_{timestamp}.json"
        raw_filepath = os.path.join(self.data_dir, "raw", raw_filename)
        
        with open(raw_filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜å»é‡å“ˆå¸Œ
        self._save_question_hashes()
        
        # æ›´æ–°çˆ¬å–å†å²
        self._update_crawl_history(len(data), raw_filepath)
        
        self.logger.info(f"æ•°æ®å·²ä¿å­˜åˆ°: {raw_filepath}")
        return raw_filepath
    
    def _update_crawl_history(self, count: int, filepath: str):
        """æ›´æ–°çˆ¬å–å†å²è®°å½•"""
        history = []
        if os.path.exists(self.history_file):
            try:
                with open(self.history_file, 'r', encoding='utf-8') as f:
                    history = json.load(f)
            except:
                pass
        
        history.append({
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'count': count,
            'filepath': filepath
        })
        
        # åªä¿ç•™æœ€è¿‘100æ¡è®°å½•
        history = history[-100:]
        
        with open(self.history_file, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
    
    def run_single_crawl(self, extract_details: bool = True, headless: bool = False) -> Optional[str]:
        """æ‰§è¡Œå•æ¬¡çˆ¬å–"""
        self.headless = headless
        
        try:
            self.setup_driver()
            
            # åŠ è½½Cookieå¹¶æ£€æŸ¥ç™»å½•çŠ¶æ€
            if self.load_cookies():
                self.driver.refresh()
                time.sleep(3)
                
                if not self.check_login_status():
                    self.logger.error("ç™»å½•çŠ¶æ€æ£€æŸ¥å¤±è´¥ï¼Œè¯·é‡æ–°ç™»å½•")
                    return None
            else:
                self.logger.error("æ— æ³•åŠ è½½Cookieï¼Œè¯·å…ˆæ‰‹åŠ¨ç™»å½•")
                return None
            
            # æ‰§è¡Œçˆ¬å–
            hot_items = self.crawl_hot_list(extract_details)
            
            if hot_items:
                filepath = self.save_data(hot_items)
                return filepath
            else:
                self.logger.warning("æœªèƒ½çˆ¬å–åˆ°æ•°æ®")
                return None
                
        except Exception as e:
            self.logger.error(f"çˆ¬å–è¿‡ç¨‹å‡ºé”™: {e}")
            return None
        finally:
            if self.driver:
                self.driver.quit()


class HotListAnalyzer:
    """çƒ­æ¦œæ•°æ®åˆ†æå™¨"""
    
    def __init__(self, data_dir: str = "data"):
        self.data_dir = data_dir
        self.raw_dir = os.path.join(data_dir, "raw")
        self.analysis_dir = os.path.join(data_dir, "analysis")
        self.reports_dir = os.path.join(data_dir, "reports")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        for directory in [self.analysis_dir, self.reports_dir]:
            os.makedirs(directory, exist_ok=True)
    
    def load_all_data(self) -> pd.DataFrame:
        """åŠ è½½æ‰€æœ‰å†å²æ•°æ®"""
        all_data = []
        
        if not os.path.exists(self.raw_dir):
            return pd.DataFrame()
        
        for filename in os.listdir(self.raw_dir):
            if filename.endswith('.json') and 'zhihu_hot' in filename:
                filepath = os.path.join(self.raw_dir, filename)
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        all_data.extend(data)
                except Exception as e:
                    print(f"åŠ è½½æ–‡ä»¶å¤±è´¥ {filename}: {e}")
        
        if not all_data:
            return pd.DataFrame()
        
        df = pd.DataFrame(all_data)
        
        # æ•°æ®ç±»å‹è½¬æ¢
        if 'crawl_time' in df.columns:
            df['crawl_time'] = pd.to_datetime(df['crawl_time'])
        if 'date' in df.columns:
            df['date'] = pd.to_datetime(df['date'])
        
        return df
    
    def analyze_hot_trends(self, days: int = 7) -> Dict:
        """åˆ†æçƒ­åº¦è¶‹åŠ¿"""
        df = self.load_all_data()
        
        if df.empty:
            return {"error": "æ²¡æœ‰å¯åˆ†æçš„æ•°æ®"}
        
        # è¿‡æ»¤æœ€è¿‘Nå¤©çš„æ•°æ®
        cutoff_date = datetime.now() - timedelta(days=days)
        if 'crawl_time' in df.columns:
            recent_df = df[df['crawl_time'] >= cutoff_date]
        else:
            recent_df = df
        
        analysis = {
            'total_questions': len(recent_df['question_hash'].unique()) if 'question_hash' in recent_df.columns else len(recent_df),
            'total_records': len(recent_df),
            'date_range': {
                'start': recent_df['crawl_time'].min().strftime('%Y-%m-%d') if 'crawl_time' in recent_df.columns else 'N/A',
                'end': recent_df['crawl_time'].max().strftime('%Y-%m-%d') if 'crawl_time' in recent_df.columns else 'N/A'
            }
        }
        
        # æ¯æ—¥é—®é¢˜æ•°é‡è¶‹åŠ¿
        if 'date' in recent_df.columns:
            daily_counts = recent_df.groupby('date').size()
            analysis['daily_trends'] = daily_counts.to_dict()
        
        # çƒ­é—¨æ ‡ç­¾åˆ†æ
        if 'question_tags' in recent_df.columns:
            all_tags = []
            for tags in recent_df['question_tags'].dropna():
                if isinstance(tags, list):
                    all_tags.extend(tags)
            
            if all_tags:
                tag_counts = pd.Series(all_tags).value_counts().head(10)
                analysis['popular_tags'] = tag_counts.to_dict()
        
        # å›ç­”æ•°åˆ†æ
        if 'answer_count' in recent_df.columns:
            answer_stats = recent_df['answer_count'].describe()
            analysis['answer_stats'] = {
                'mean': round(answer_stats['mean'], 2),
                'median': answer_stats['50%'],
                'max': answer_stats['max'],
                'min': answer_stats['min']
            }
        
        # æ’åç¨³å®šæ€§åˆ†æï¼ˆåŒä¸€é—®é¢˜åœ¨ä¸åŒæ—¶é—´çš„æ’åå˜åŒ–ï¼‰
        if 'question_hash' in recent_df.columns and 'rank' in recent_df.columns:
            rank_stability = recent_df.groupby('question_hash')['rank'].agg(['count', 'mean', 'std']).reset_index()
            rank_stability = rank_stability[rank_stability['count'] > 1]  # åªåˆ†æå‡ºç°å¤šæ¬¡çš„é—®é¢˜
            
            if not rank_stability.empty:
                analysis['rank_stability'] = {
                    'stable_questions': len(rank_stability[rank_stability['std'] < 5]),  # æ’åå˜åŒ–å°äº5
                    'volatile_questions': len(rank_stability[rank_stability['std'] >= 10]),  # æ’åå˜åŒ–å¤§äº10
                    'avg_rank_change': round(rank_stability['std'].mean(), 2)
                }
        
        return analysis
    

    def generate_trend_charts(self, days: int = 7):
        """ç”Ÿæˆ6ä¸ªç‹¬ç«‹çš„åˆ†æå›¾è¡¨"""
        df = self.load_all_data()
        
        if df.empty:
            print("âŒ æ²¡æœ‰æ•°æ®å¯ç”¨äºç”Ÿæˆå›¾è¡¨")
            return None
        
        # è¿‡æ»¤æ•°æ®
        cutoff_date = datetime.now() - timedelta(days=days)
        if 'crawl_time' in df.columns:
            recent_df = df[df['crawl_time'] >= cutoff_date]
        else:
            recent_df = df
        
        if recent_df.empty:
            print(f"âŒ æœ€è¿‘{days}å¤©æ²¡æœ‰æ•°æ®")
            return None
        
        print(f"ğŸ“Š æ­£åœ¨ç”Ÿæˆæœ€è¿‘{days}å¤©çš„6ä¸ªç‹¬ç«‹åˆ†æå›¾è¡¨...")
        
        # è®¾ç½®matplotlibå‚æ•°
        plt.rcParams.update({
            'font.sans-serif': ['Microsoft YaHei', 'SimHei', 'Arial', 'DejaVu Sans'],
            'axes.unicode_minus': False,
            'figure.dpi': 100,
            'savefig.dpi': 300,
            'font.size': 14,
            'axes.titlesize': 18,
            'axes.labelsize': 14,
            'xtick.labelsize': 12,
            'ytick.labelsize': 12,
            'legend.fontsize': 12
        })
        
        # é¢œè‰²é…ç½®
        colors = {
            'primary': '#3498db',
            'secondary': '#e74c3c', 
            'success': '#2ecc71',
            'warning': '#f39c12',
            'info': '#9b59b6',
            'dark': '#34495e'
        }
        
        chart_files = []
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # =================== å›¾è¡¨1: æ¯æ—¥æ•°æ®é‡‡é›†è¶‹åŠ¿ ===================
        plt.figure(figsize=(12, 8))
        if 'date' in recent_df.columns:
            daily_counts = recent_df.groupby('date').size()
            
            plt.plot(daily_counts.index, daily_counts.values, 
                    marker='o', linewidth=4, markersize=12, 
                    color=colors['primary'], markerfacecolor='white', 
                    markeredgewidth=3, markeredgecolor=colors['primary'],
                    label='æ¯æ—¥é—®é¢˜æ•°')
            
            plt.fill_between(daily_counts.index, daily_counts.values, 
                            alpha=0.3, color=colors['primary'])
            
            # æ·»åŠ è¶‹åŠ¿çº¿
            if len(daily_counts) > 1:
                z = np.polyfit(range(len(daily_counts)), daily_counts.values, 1)
                p = np.poly1d(z)
                plt.plot(daily_counts.index, p(range(len(daily_counts))), 
                        "--", alpha=0.8, color=colors['secondary'], linewidth=3,
                        label='è¶‹åŠ¿çº¿')
            
            # æ ‡æ³¨æœ€é«˜ç‚¹
            max_idx = daily_counts.idxmax()
            max_val = daily_counts.max()
            plt.annotate(f'å³°å€¼\n{max_val}ä¸ªé—®é¢˜', 
                        xy=(max_idx, max_val),
                        xytext=(max_idx, max_val + max_val*0.1),
                        arrowprops=dict(arrowstyle='->', color=colors['secondary'], lw=2),
                        fontweight='bold', ha='center', fontsize=12,
                        bbox=dict(boxstyle="round,pad=0.5", facecolor='yellow', alpha=0.8))
            
            plt.title(f'æ¯æ—¥æ•°æ®é‡‡é›†è¶‹åŠ¿ - æœ€è¿‘{days}å¤©', fontweight='bold', fontsize=20, pad=20)
            plt.xlabel('æ—¥æœŸ', fontweight='bold')
            plt.ylabel('é—®é¢˜æ•°é‡', fontweight='bold')
            plt.grid(True, alpha=0.3, linestyle='--')
            plt.legend()
            plt.xticks(rotation=45)
        else:
            plt.text(0.5, 0.5, 'æš‚æ— æ—¥æœŸæ•°æ®', transform=plt.gca().transAxes,
                    ha='center', va='center', fontsize=20, color='gray')
            plt.title(f'æ¯æ—¥æ•°æ®é‡‡é›†è¶‹åŠ¿ - æœ€è¿‘{days}å¤©', fontweight='bold', fontsize=20, pad=20)
        
        plt.tight_layout()
        chart1_path = os.path.join(self.analysis_dir, f'chart1_daily_trend_{timestamp}.png')
        plt.savefig(chart1_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        chart_files.append(chart1_path)
        print(f"âœ… å›¾è¡¨1å·²ä¿å­˜: {chart1_path}")
        
        # =================== å›¾è¡¨2: æ’ååŒºé—´åˆ†å¸ƒ ===================
        plt.figure(figsize=(12, 8))
        if 'rank' in recent_df.columns:
            rank_ranges = ['1-10å\n(çƒ­é—¨)', '11-20å\n(ä¼˜ç§€)', '21-30å\n(è‰¯å¥½)', '31-40å\n(ä¸€èˆ¬)', '41-50å\n(æ™®é€š)']
            range_counts = []
            
            for i in range(5):
                start = i * 10 + 1
                end = (i + 1) * 10
                count = recent_df[recent_df['rank'].between(start, end)]['rank'].count()
                range_counts.append(count)
            
            colors_list = [colors['success'], colors['primary'], colors['warning'], 
                        colors['secondary'], colors['info']]
            
            bars = plt.bar(rank_ranges, range_counts, color=colors_list, 
                        alpha=0.8, edgecolor='white', linewidth=3)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, count in zip(bars, range_counts):
                if count > 0:
                    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                            f'{count}ä¸ª', ha='center', va='bottom', fontweight='bold', fontsize=14)
            
            plt.title(f'æ’ååŒºé—´åˆ†å¸ƒ - æœ€è¿‘{days}å¤©', fontweight='bold', fontsize=20, pad=20)
            plt.xlabel('æ’ååŒºé—´', fontweight='bold')
            plt.ylabel('é—®é¢˜æ•°é‡', fontweight='bold')
            plt.grid(True, alpha=0.3, axis='y')
        else:
            plt.text(0.5, 0.5, 'æš‚æ— æ’åæ•°æ®', transform=plt.gca().transAxes,
                    ha='center', va='center', fontsize=20, color='gray')
            plt.title(f'æ’ååŒºé—´åˆ†å¸ƒ - æœ€è¿‘{days}å¤©', fontweight='bold', fontsize=20, pad=20)
        
        plt.tight_layout()
        chart2_path = os.path.join(self.analysis_dir, f'chart2_rank_distribution_{timestamp}.png')
        plt.savefig(chart2_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        chart_files.append(chart2_path)
        print(f"âœ… å›¾è¡¨2å·²ä¿å­˜: {chart2_path}")
        
        # =================== å›¾è¡¨3: å›ç­”æ•°åˆ†å¸ƒåˆ†æ ===================
        plt.figure(figsize=(12, 8))
        if 'answer_count' in recent_df.columns and not recent_df['answer_count'].isna().all():
            answer_counts = recent_df['answer_count'].dropna()
            if not answer_counts.empty and answer_counts.sum() > 0:
                # ç»˜åˆ¶ç›´æ–¹å›¾
                n, bins, patches = plt.hist(answer_counts, bins=30, 
                                        color=colors['success'], alpha=0.7, 
                                        edgecolor='white', linewidth=1)
                
                # æ·»åŠ ç»Ÿè®¡çº¿
                mean_val = answer_counts.mean()
                median_val = answer_counts.median()
                plt.axvline(mean_val, color=colors['secondary'], linestyle='--', 
                        linewidth=4, label=f'å‡å€¼: {mean_val:.1f}ä¸ªå›ç­”')
                plt.axvline(median_val, color=colors['warning'], linestyle='--', 
                        linewidth=4, label=f'ä¸­ä½æ•°: {median_val:.1f}ä¸ªå›ç­”')
                
                # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
                stats_text = f'æ€»é—®é¢˜æ•°: {len(answer_counts)}\næœ€å¤šå›ç­”: {answer_counts.max()}ä¸ª\næœ€å°‘å›ç­”: {answer_counts.min()}ä¸ª'
                plt.text(0.98, 0.98, stats_text, transform=plt.gca().transAxes,
                        verticalalignment='top', horizontalalignment='right',
                        bbox=dict(boxstyle="round,pad=0.5", facecolor='lightblue', alpha=0.8),
                        fontsize=12, fontweight='bold')
                
                plt.title(f'å›ç­”æ•°åˆ†å¸ƒåˆ†æ - æœ€è¿‘{days}å¤©', fontweight='bold', fontsize=20, pad=20)
                plt.xlabel('å›ç­”æ•°', fontweight='bold')
                plt.ylabel('é—®é¢˜æ•°é‡', fontweight='bold')
                plt.legend(loc='upper right')
                plt.grid(True, alpha=0.3)
            else:
                plt.text(0.5, 0.5, 'æš‚æ— æœ‰æ•ˆå›ç­”æ•°æ®', transform=plt.gca().transAxes,
                        ha='center', va='center', fontsize=20, color='gray')
        else:
            plt.text(0.5, 0.5, 'æš‚æ— å›ç­”æ•°æ®', transform=plt.gca().transAxes,
                    ha='center', va='center', fontsize=20, color='gray')
            plt.title(f'å›ç­”æ•°åˆ†å¸ƒåˆ†æ - æœ€è¿‘{days}å¤©', fontweight='bold', fontsize=20, pad=20)
        
        plt.tight_layout()
        chart3_path = os.path.join(self.analysis_dir, f'chart3_answer_distribution_{timestamp}.png')
        plt.savefig(chart3_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        chart_files.append(chart3_path)
        print(f"âœ… å›¾è¡¨3å·²ä¿å­˜: {chart3_path}")
        
        # =================== å›¾è¡¨4: çƒ­é—¨è¯é¢˜æ ‡ç­¾ ===================
        plt.figure(figsize=(12, 10))
        if 'question_tags' in recent_df.columns:
            all_tags = []
            for tags in recent_df['question_tags'].dropna():
                if isinstance(tags, list):
                    all_tags.extend(tags)
            
            if all_tags:
                tag_counts = pd.Series(all_tags).value_counts().head(15)
                
                # åˆ›å»ºæ¸å˜è‰²
                colors_gradient = plt.cm.Set3(np.linspace(0, 1, len(tag_counts)))
                
                bars = plt.barh(range(len(tag_counts)), tag_counts.values, 
                            color=colors_gradient, alpha=0.8, 
                            edgecolor='white', linewidth=2)
                
                plt.yticks(range(len(tag_counts)), tag_counts.index, fontsize=12)
                plt.title(f'çƒ­é—¨è¯é¢˜æ ‡ç­¾æ’è¡Œæ¦œ - æœ€è¿‘{days}å¤©', fontweight='bold', fontsize=20, pad=20)
                plt.xlabel('å‡ºç°æ¬¡æ•°', fontweight='bold')
                plt.grid(True, alpha=0.3, axis='x')
                
                # æ·»åŠ æ•°å€¼æ ‡ç­¾
                for i, (bar, count) in enumerate(zip(bars, tag_counts.values)):
                    plt.text(bar.get_width() + max(tag_counts.values) * 0.01, 
                            bar.get_y() + bar.get_height()/2,
                            f'{count}æ¬¡', ha='left', va='center', fontweight='bold', fontsize=11)
                
                # æ·»åŠ æ’åæ ‡ç­¾
                for i, bar in enumerate(bars):
                    plt.text(bar.get_width() * 0.05, bar.get_y() + bar.get_height()/2,
                            f'#{i+1}', ha='left', va='center', fontweight='bold', 
                            fontsize=10, color='white')
            else:
                plt.text(0.5, 0.5, 'æš‚æ— æ ‡ç­¾æ•°æ®', transform=plt.gca().transAxes,
                        ha='center', va='center', fontsize=20, color='gray')
        else:
            plt.text(0.5, 0.5, 'æš‚æ— æ ‡ç­¾æ•°æ®', transform=plt.gca().transAxes,
                    ha='center', va='center', fontsize=20, color='gray')
            plt.title(f'çƒ­é—¨è¯é¢˜æ ‡ç­¾æ’è¡Œæ¦œ - æœ€è¿‘{days}å¤©', fontweight='bold', fontsize=20, pad=20)
        
        plt.tight_layout()
        chart4_path = os.path.join(self.analysis_dir, f'chart4_popular_tags_{timestamp}.png')
        plt.savefig(chart4_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        chart_files.append(chart4_path)
        print(f"âœ… å›¾è¡¨4å·²ä¿å­˜: {chart4_path}")
        
        # =================== å›¾è¡¨5: 24å°æ—¶æ´»è·ƒåº¦åˆ†å¸ƒ ===================
        plt.figure(figsize=(14, 8))
        if 'crawl_time' in recent_df.columns:
            df_hour = recent_df.copy()
            df_hour['hour'] = df_hour['crawl_time'].dt.hour
            hourly_counts = df_hour.groupby('hour').size()
            
            # åˆ›å»º24å°æ—¶å®Œæ•´æ•°æ®
            full_hours = pd.Series(0, index=range(24))
            full_hours.update(hourly_counts)
            
            # åˆ›å»ºæ¸å˜è‰²æ•ˆæœ
            colors_hour = []
            max_count = full_hours.max() if full_hours.max() > 0 else 1
            for count in full_hours.values:
                intensity = count / max_count
                colors_hour.append(plt.cm.viridis(intensity))
            
            bars = plt.bar(full_hours.index, full_hours.values, 
                        color=colors_hour, alpha=0.8, 
                        edgecolor='white', linewidth=1.5)
            
            # æ ‡æ³¨peakæ—¶æ®µ
            if not hourly_counts.empty:
                peak_hour = hourly_counts.idxmax()
                peak_count = hourly_counts.max()
                plt.annotate(f'é«˜å³°æ—¶æ®µ\n{peak_hour}:00\næ´»è·ƒåº¦: {peak_count}', 
                            xy=(peak_hour, peak_count),
                            xytext=(peak_hour+3, peak_count+2),
                            arrowprops=dict(arrowstyle='->', color=colors['secondary'], lw=3),
                            fontweight='bold', ha='center', fontsize=12,
                            bbox=dict(boxstyle="round,pad=0.5", facecolor='yellow', alpha=0.9))
            
            # æ—¶æ®µæ ‡æ³¨
            time_periods = {
                (0, 6): 'æ·±å¤œæ—¶æ®µ',
                (6, 12): 'ä¸Šåˆæ—¶æ®µ', 
                (12, 18): 'ä¸‹åˆæ—¶æ®µ',
                (18, 24): 'æ™šé—´æ—¶æ®µ'
            }
            
            for (start, end), period in time_periods.items():
                avg_activity = full_hours[start:end].mean()
                mid_hour = (start + end) // 2
                plt.text(mid_hour, avg_activity + max_count * 0.1, period,
                        ha='center', va='bottom', fontweight='bold', fontsize=10,
                        bbox=dict(boxstyle="round,pad=0.3", facecolor='lightgray', alpha=0.7))
            
            plt.title(f'24å°æ—¶æ´»è·ƒåº¦åˆ†å¸ƒ - æœ€è¿‘{days}å¤©', fontweight='bold', fontsize=20, pad=20)
            plt.xlabel('å°æ—¶', fontweight='bold')
            plt.ylabel('æ´»è·ƒåº¦', fontweight='bold')
            plt.xticks(range(0, 24, 2), [f'{h}:00' for h in range(0, 24, 2)])
            plt.grid(True, alpha=0.3, axis='y')
        else:
            plt.text(0.5, 0.5, 'æš‚æ— æ—¶é—´æ•°æ®', transform=plt.gca().transAxes,
                    ha='center', va='center', fontsize=20, color='gray')
            plt.title(f'24å°æ—¶æ´»è·ƒåº¦åˆ†å¸ƒ - æœ€è¿‘{days}å¤©', fontweight='bold', fontsize=20, pad=20)
        
        plt.tight_layout()
        chart5_path = os.path.join(self.analysis_dir, f'chart5_hourly_activity_{timestamp}.png')
        plt.savefig(chart5_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        chart_files.append(chart5_path)
        print(f"âœ… å›¾è¡¨5å·²ä¿å­˜: {chart5_path}")
        
        # =================== å›¾è¡¨6: æ•°æ®è´¨é‡ä¸ç»Ÿè®¡æŒ‡æ ‡ ===================
        plt.figure(figsize=(12, 8))
        
        # è®¡ç®—æŒ‡æ ‡
        total_records = len(recent_df)
        unique_questions = recent_df['question_hash'].nunique() if 'question_hash' in recent_df.columns else len(recent_df)
        
        # æ•°æ®å®Œæ•´ç‡
        completeness_scores = []
        if 'answer_count' in recent_df.columns:
            answer_completeness = (recent_df['answer_count'].notna().sum() / len(recent_df)) * 100
            completeness_scores.append(('å›ç­”æ•°å®Œæ•´ç‡', answer_completeness))
        
        if 'question_tags' in recent_df.columns:
            tag_completeness = (recent_df['question_tags'].apply(
                lambda x: len(x) > 0 if isinstance(x, list) else False).sum() / len(recent_df)) * 100
            completeness_scores.append(('æ ‡ç­¾å®Œæ•´ç‡', tag_completeness))
        
        # é‡å¤ç‡
        duplicate_rate = ((total_records - unique_questions) / total_records * 100) if total_records > 0 else 0
        
        # ç»˜åˆ¶åœ†é¥¼å›¾æ˜¾ç¤ºæ•°æ®è´¨é‡
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
        
        # å·¦å›¾ï¼šæ•°æ®å®Œæ•´æ€§
        if completeness_scores:
            labels = [item[0] for item in completeness_scores]
            sizes = [item[1] for item in completeness_scores]
            colors_pie = [colors['success'], colors['info']][:len(sizes)]
            
            ax1.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90,
                colors=colors_pie, textprops={'fontsize': 12, 'fontweight': 'bold'})
            ax1.set_title('æ•°æ®å®Œæ•´æ€§åˆ†æ', fontweight='bold', fontsize=16, pad=20)
        
        # å³å›¾ï¼šå…³é”®æŒ‡æ ‡
        metrics = ['æ€»è®°å½•æ•°', 'ç‹¬ç‰¹é—®é¢˜', f'é‡å¤ç‡({duplicate_rate:.1f}%)']
        values = [total_records, unique_questions, duplicate_rate]
        colors_metrics = [colors['primary'], colors['success'], colors['warning']]
        
        bars = ax2.bar(metrics, values, color=colors_metrics, 
                    alpha=0.8, edgecolor='white', linewidth=2)
        
        ax2.set_title('å…³é”®ç»Ÿè®¡æŒ‡æ ‡', fontweight='bold', fontsize=16, pad=20)
        ax2.set_ylabel('æ•°å€¼', fontweight='bold')
        ax2.grid(True, alpha=0.3, axis='y')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value, metric in zip(bars, values, metrics):
            if 'ç‡' in metric:
                label = f'{value:.1f}%'
            else:
                label = f'{int(value):,}'
            
            ax2.text(bar.get_x() + bar.get_width()/2, 
                    bar.get_height() + max(values) * 0.02,
                    label, ha='center', va='bottom', fontweight='bold', fontsize=12)
        
        plt.suptitle(f'æ•°æ®è´¨é‡ä¸ç»Ÿè®¡åˆ†æ - æœ€è¿‘{days}å¤©', fontweight='bold', fontsize=20, y=0.95)
        plt.tight_layout()
        chart6_path = os.path.join(self.analysis_dir, f'chart6_data_quality_{timestamp}.png')
        plt.savefig(chart6_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        chart_files.append(chart6_path)
        print(f"âœ… å›¾è¡¨6å·²ä¿å­˜: {chart6_path}")
        
        # ç”Ÿæˆæ±‡æ€»ä¿¡æ¯
        print(f"\nğŸ‰ æ‰€æœ‰å›¾è¡¨ç”Ÿæˆå®Œæˆï¼")
        print(f"ğŸ“ ä¿å­˜ä½ç½®: {self.analysis_dir}")
        print(f"ğŸ“Š å›¾è¡¨åˆ—è¡¨:")
        for i, chart_file in enumerate(chart_files, 1):
            filename = os.path.basename(chart_file)
            print(f"   {i}. {filename}")
        
        print(f"\nğŸ“ˆ æ•°æ®æ¦‚è§ˆ:")
        print(f"   â€¢ åˆ†æå‘¨æœŸ: {days} å¤©")
        print(f"   â€¢ æ€»è®°å½•æ•°: {total_records:,} æ¡")
        print(f"   â€¢ ç‹¬ç‰¹é—®é¢˜: {unique_questions:,} ä¸ª")
        print(f"   â€¢ ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        return chart_files
    
    def generate_report(self, days: int = 7) -> str:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        analysis = self.analyze_hot_trends(days)
        
        if "error" in analysis:
            return analysis["error"]
        
        report_content = f"""
# çŸ¥ä¹çƒ­æ¦œæ•°æ®åˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**åˆ†æå‘¨æœŸ**: æœ€è¿‘{days}å¤©

## ğŸ“Š æ•°æ®æ¦‚è§ˆ

- **æ€»é—®é¢˜æ•°**: {analysis.get('total_questions', 'N/A')} ä¸ªç‹¬ç‰¹é—®é¢˜
- **æ€»è®°å½•æ•°**: {analysis.get('total_records', 'N/A')} æ¡è®°å½•
- **æ•°æ®æ—¶é—´èŒƒå›´**: {analysis.get('date_range', {}).get('start', 'N/A')} è‡³ {analysis.get('date_range', {}).get('end', 'N/A')}

## ğŸ“ˆ è¶‹åŠ¿åˆ†æ

### æ¯æ—¥æ´»è·ƒåº¦
"""
        
        if 'daily_trends' in analysis:
            report_content += "\n| æ—¥æœŸ | é—®é¢˜æ•°é‡ |\n|------|----------|\n"
            for date, count in analysis['daily_trends'].items():
                report_content += f"| {date} | {count} |\n"
        
        if 'popular_tags' in analysis:
            report_content += "\n\n### ğŸ·ï¸ çƒ­é—¨æ ‡ç­¾\n\n"
            for tag, count in analysis['popular_tags'].items():
                report_content += f"- **{tag}**: {count} æ¬¡\n"
        
        if 'answer_stats' in analysis:
            stats = analysis['answer_stats']
            report_content += f"""

### ğŸ“ å›ç­”æ•°æ®ç»Ÿè®¡

- **å¹³å‡å›ç­”æ•°**: {stats['mean']} ä¸ª
- **ä¸­ä½æ•°å›ç­”æ•°**: {stats['median']} ä¸ª  
- **æœ€å¤šå›ç­”æ•°**: {stats['max']} ä¸ª
- **æœ€å°‘å›ç­”æ•°**: {stats['min']} ä¸ª
"""
        
        if 'rank_stability' in analysis:
            stability = analysis['rank_stability']
            report_content += f"""

### ğŸ“Š æ’åç¨³å®šæ€§åˆ†æ

- **ç¨³å®šé—®é¢˜æ•°** (æ’åå˜åŒ–<5): {stability['stable_questions']} ä¸ª
- **æ³¢åŠ¨é—®é¢˜æ•°** (æ’åå˜åŒ–â‰¥10): {stability['volatile_questions']} ä¸ª
- **å¹³å‡æ’åå˜åŒ–**: {stability['avg_rank_change']}
"""
        
        # ä¿å­˜æŠ¥å‘Š
        report_filename = f"analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        report_path = os.path.join(self.reports_dir, report_filename)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        return report_path


class ScheduledCrawler:
    """å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨"""
    
    def __init__(self, data_dir: str = "data"):
        self.crawler = EnhancedZhihuCrawler(data_dir)
        self.analyzer = HotListAnalyzer(data_dir)
        self.scheduler = BlockingScheduler()
        
        # é…ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
    
    def scheduled_crawl_job(self, extract_details: bool = True):
        """å®šæ—¶çˆ¬å–ä»»åŠ¡"""
        self.logger.info("å¼€å§‹æ‰§è¡Œå®šæ—¶çˆ¬å–ä»»åŠ¡")
        
        try:
            filepath = self.crawler.run_single_crawl(extract_details=extract_details, headless=True)
            if filepath:
                self.logger.info(f"å®šæ—¶çˆ¬å–å®Œæˆï¼Œæ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
                
                # æ¯å¤©æ™šä¸Šç”Ÿæˆåˆ†ææŠ¥å‘Š
                current_hour = datetime.now().hour
                if current_hour == 23:  # æ™šä¸Š11ç‚¹ç”ŸæˆæŠ¥å‘Š
                    self.logger.info("ç”Ÿæˆæ¯æ—¥åˆ†ææŠ¥å‘Š")
                    self.analyzer.generate_report(days=1)
                    
            else:
                self.logger.error("å®šæ—¶çˆ¬å–å¤±è´¥")
                
        except Exception as e:
            self.logger.error(f"å®šæ—¶ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
    
    def scheduled_analysis_job(self, days: int = 7):
        """å®šæ—¶åˆ†æä»»åŠ¡"""
        self.logger.info("å¼€å§‹æ‰§è¡Œå®šæ—¶åˆ†æä»»åŠ¡")
        
        try:
            # ç”Ÿæˆåˆ†ææŠ¥å‘Š
            report_path = self.analyzer.generate_report(days=days)
            self.logger.info(f"åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
            
            # ç”Ÿæˆå›¾è¡¨
            self.analyzer.generate_trend_charts(days=days)
            self.logger.info("è¶‹åŠ¿å›¾è¡¨å·²ç”Ÿæˆ")
            
        except Exception as e:
            self.logger.error(f"åˆ†æä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
    
    def add_crawl_jobs(self):
        """æ·»åŠ çˆ¬å–ä»»åŠ¡"""
        # æ¯2å°æ—¶çˆ¬å–ä¸€æ¬¡ï¼ˆåŸºç¡€ä¿¡æ¯ï¼‰
        self.scheduler.add_job(
            func=self.scheduled_crawl_job,
            trigger=CronTrigger(minute=0, second=0, hour='*/2'),
            args=[False],  # ä¸æå–è¯¦ç»†ä¿¡æ¯ï¼Œæé«˜é€Ÿåº¦
            id='crawl_basic_2h',
            name='æ¯2å°æ—¶åŸºç¡€çˆ¬å–',
            misfire_grace_time=300
        )
        
        # æ¯6å°æ—¶è¯¦ç»†çˆ¬å–ä¸€æ¬¡ï¼ˆåŒ…å«è¯¦ç»†ä¿¡æ¯ï¼‰
        self.scheduler.add_job(
            func=self.scheduled_crawl_job,
            trigger=CronTrigger(minute=30, second=0, hour='*/6'),
            args=[True],  # æå–è¯¦ç»†ä¿¡æ¯
            id='crawl_detailed_6h',
            name='æ¯6å°æ—¶è¯¦ç»†çˆ¬å–',
            misfire_grace_time=600
        )
        
        self.logger.info("çˆ¬å–ä»»åŠ¡å·²æ·»åŠ ")
    
    def add_analysis_jobs(self):
        """æ·»åŠ åˆ†æä»»åŠ¡"""
        # æ¯å¤©æ—©ä¸Š8ç‚¹ç”Ÿæˆå‘¨æŠ¥å‘Š
        self.scheduler.add_job(
            func=self.scheduled_analysis_job,
            trigger=CronTrigger(hour=8, minute=0, second=0),
            args=[7],  # 7å¤©æŠ¥å‘Š
            id='weekly_analysis',
            name='æ¯æ—¥å‘¨æŠ¥å‘Šç”Ÿæˆ',
            misfire_grace_time=3600
        )
        
        # æ¯å‘¨ä¸€æ—©ä¸Š9ç‚¹ç”ŸæˆæœˆæŠ¥å‘Š
        self.scheduler.add_job(
            func=self.scheduled_analysis_job,
            trigger=CronTrigger(day_of_week='mon', hour=9, minute=0, second=0),
            args=[30],  # 30å¤©æŠ¥å‘Š
            id='monthly_analysis',
            name='æ¯å‘¨æœˆæŠ¥å‘Šç”Ÿæˆ',
            misfire_grace_time=3600
        )
        
        self.logger.info("åˆ†æä»»åŠ¡å·²æ·»åŠ ")
    
    def start_scheduler(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        self.add_crawl_jobs()
        self.add_analysis_jobs()
        
        self.logger.info("å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å¯åŠ¨")
        self.logger.info("å·²æ·»åŠ çš„ä»»åŠ¡:")
        for job in self.scheduler.get_jobs():
            self.logger.info(f"  - {job.name} (ID: {job.id})")
        
        try:
            self.scheduler.start()
        except KeyboardInterrupt:
            self.logger.info("æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å…³é—­è°ƒåº¦å™¨...")
            self.scheduler.shutdown()
    
    def run_manual_analysis(self, days: int = 7):
        """æ‰‹åŠ¨è¿è¡Œåˆ†æ"""
        self.logger.info(f"å¼€å§‹æ‰‹åŠ¨åˆ†ææœ€è¿‘{days}å¤©çš„æ•°æ®")
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        report_path = self.analyzer.generate_report(days=days)
        print(f"\nâœ“ åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        
        # ç”Ÿæˆè¶‹åŠ¿å›¾è¡¨
        self.analyzer.generate_trend_charts(days=days)
        print("âœ“ è¶‹åŠ¿å›¾è¡¨å·²ç”Ÿæˆ")
        
        # æ˜¾ç¤ºå…³é”®ç»Ÿè®¡ä¿¡æ¯
        analysis = self.analyzer.analyze_hot_trends(days=days)
        if "error" not in analysis:
            print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡ (æœ€è¿‘{days}å¤©):")
            print(f"  - æ€»é—®é¢˜æ•°: {analysis.get('total_questions', 'N/A')}")
            print(f"  - æ€»è®°å½•æ•°: {analysis.get('total_records', 'N/A')}")
            
            if 'popular_tags' in analysis:
                print("  - çƒ­é—¨æ ‡ç­¾:")
                for tag, count in list(analysis['popular_tags'].items())[:5]:
                    print(f"    * {tag}: {count}æ¬¡")


def main():
    """ä¸»å‡½æ•° - å¢å¼ºç‰ˆåŠŸèƒ½æ¼”ç¤º"""
    print("="*60)
    print("çŸ¥ä¹çƒ­æ¦œçˆ¬è™«ç³»ç»Ÿ - å¢å¼ºç‰ˆ")
    print("="*60)
    print("åŠŸèƒ½:")
    print("1. å•æ¬¡çˆ¬å– (åŸºç¡€)")
    print("2. å•æ¬¡çˆ¬å– (è¯¦ç»†ä¿¡æ¯)")
    print("3. æ•°æ®åˆ†æ")
    print("4. å¯åŠ¨å®šæ—¶ä»»åŠ¡")
    print("5. ç”Ÿæˆè¶‹åŠ¿å›¾è¡¨")
    print("6. é€€å‡º")
    print("="*60)
    
    crawler = EnhancedZhihuCrawler()
    analyzer = HotListAnalyzer()
    scheduler = ScheduledCrawler()
    
    while True:
        try:
            choice = input("\nè¯·é€‰æ‹©åŠŸèƒ½ (1-6): ").strip()
            
            if choice == '1':
                print("\nå¼€å§‹åŸºç¡€çˆ¬å–...")
                filepath = crawler.run_single_crawl(extract_details=False)
                if filepath:
                    print(f"âœ“ çˆ¬å–å®Œæˆï¼Œæ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
                else:
                    print("âœ— çˆ¬å–å¤±è´¥")
            
            elif choice == '2':
                print("\nå¼€å§‹è¯¦ç»†çˆ¬å–ï¼ˆåŒ…å«å›ç­”æ•°ã€æµè§ˆæ•°ç­‰ï¼‰...")
                filepath = crawler.run_single_crawl(extract_details=True)
                if filepath:
                    print(f"âœ“ çˆ¬å–å®Œæˆï¼Œæ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
                else:
                    print("âœ— çˆ¬å–å¤±è´¥")
            
            elif choice == '3':
                days = input("è¯·è¾“å…¥åˆ†æå¤©æ•° (é»˜è®¤7å¤©): ").strip()
                days = int(days) if days.isdigit() else 7
                scheduler.run_manual_analysis(days)
            
            elif choice == '4':
                print("\nå¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨...")
                print("ä»»åŠ¡é…ç½®:")
                print("  - æ¯2å°æ—¶åŸºç¡€çˆ¬å–")
                print("  - æ¯6å°æ—¶è¯¦ç»†çˆ¬å–") 
                print("  - æ¯å¤©8ç‚¹ç”Ÿæˆå‘¨æŠ¥å‘Š")
                print("  - æ¯å‘¨ä¸€9ç‚¹ç”ŸæˆæœˆæŠ¥å‘Š")
                print("\næŒ‰ Ctrl+C åœæ­¢è°ƒåº¦å™¨")
                scheduler.start_scheduler()
            
            elif choice == '5':
                days = input("è¯·è¾“å…¥åˆ†æå¤©æ•° (é»˜è®¤7å¤©): ").strip()
                days = int(days) if days.isdigit() else 7
                analyzer.generate_trend_charts(days)
            
            elif choice == '6':
                print("ç¨‹åºé€€å‡º")
                break
            
            else:
                print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
                
        except KeyboardInterrupt:
            print("\n\nç¨‹åºè¢«ä¸­æ–­")
            break
        except Exception as e:
            print(f"å‘ç”Ÿé”™è¯¯: {e}")


if __name__ == "__main__":
    main()